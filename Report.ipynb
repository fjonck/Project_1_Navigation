{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The learning algorithm\n",
    "\n",
    "The learning algorithm I implemented is based on the [Udacity exercise](https://github.com/udacity/deep-reinforcement-learning/tree/master/dqn) to implement Deep Q-Learning to solve the [OpenAI Gym's LunarLander environment](https://gym.openai.com/envs/LunarLander-v2/). The implementation is a vanilla Deep Q-Network based on the paper [\"Human-level control through deep reinforcement learning\"](http://dx.doi.org/10.1038/nature14236)[^1] \n",
    "\n",
    "### The implementation details\n",
    "\n",
    "The implementation at hand is a *Q-Learning algorithm* with a *Deep Neural Network* to approximate the optimal action-value function $Q^*(s,a)$. We are using the L2-Norm for computing the loss and the Adam optimizer[^2]. Additionally we use the two features *Experience Replay* and *Fixed Q-Targets*. The following pseudo-code of the algorithm is a variation of the combination of the paper [\"Human-level control through deep reinforcement learning\"](http://dx.doi.org/10.1038/nature14236)[^1] and the [Udacity cheatsheet](https://github.com/udacity/deep-reinforcement-learning/blob/master/cheatsheet/cheatsheet.pdf):\n",
    "\n",
    "Input: policy $\\pi$, positive integer *num_episodes*, small positive fraction $\\alpha$, GLIE {$\\epsilon_i$} with update rule $\\epsilon_{i+1} \\leftarrow \\max(\\epsilon_{decay}*\\epsilon_i, \\epsilon_{min})$\n",
    "Output: value function $Q$ ($\\approx q_\\pi$ if *num_episodes* is large enough)\n",
    "\n",
    "Initialize action-value function $Q(s,a)$ with random weights $\\Theta$ for all $s \\in S$ and $a \\in A(s)$.\n",
    "Initialize target action-value function $\\tilde{Q}(s,a)$ with weights $\\tilde{\\Theta} = \\Theta$ from $Q(s,a)$ for all $s \\in S$ and $a \\in A(s)$.\n",
    "Initialize replay memory $D$ to capacity $N$  \n",
    "\n",
    "**for** $i \\leftarrow 1$ to *num_episodes* **do** \n",
    ">$\\epsilon \\leftarrow \\epsilon_i$  \n",
    "Observe $S_0$  \n",
    "$t \\leftarrow 0$  \n",
    "**repeat**  \n",
    ">>Choose action $A_t$ using an $\\epsilon$-greedy policy derived from $Q$  \n",
    "Take action $A_t$ and observe $R_{t+1} , S_{t+1}$  \n",
    "Store experience $(S_t, A_t, R_t, R_{t+1})$ in $D$  \n",
    "**Every** C steps **do**  \n",
    ">>>Sample a random minibatch of experience tuples $(S_t, A_t, R_t, R_{t+1})$ from $D$  \n",
    "Set $Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha(R_{t+1} + \\gamma \\max_a \\tilde{Q}(S_{t+1}, a)-Q(S_t, A_t))$  \n",
    "Perform back propagation according to the L2-Norm  \n",
    "and the Adam-optimizer to update the weights of the action-value function $Q$  \n",
    "Update $\\tilde{\\Theta} \\leftarrow \\tau*\\Theta + (1 - \\tau)*\\tilde{\\Theta}$  \n",
    "\n",
    ">>**end**  \n",
    "$t \\leftarrow t+1$  \n",
    "\n",
    ">**until** $S_t$ is terminal;  \n",
    "\n",
    "**end**  \n",
    "**return** $Q$  \n",
    "\n",
    "We use *Experience Replay* to prevent correlations of successive experiences. We can make use of rare occurrences of experiences and learn from them multiple times.  At every time step $t$ we store the experience tuple $(s_j, a_j, r_j, s_{j+1})$ in the replay buffer $D$ and sample from it a random minibatch to update the network. \n",
    "We use *Fixed Q-Targets* to prevent correlations which come the fact that we try to update the approximated action-value function $Q$ with an approximated action-value function $Q$. To make use of the feature of fixed Q-targets, we introduce a target action-value function $\\tilde{Q}$ and use it in the update rule $Q(S_t, A_t) = Q(S_t, A_t) + \\alpha(R_{t+1} + \\gamma \\max_a \\tilde{Q}(S_{t+1}, a) âˆ’ Q(S_t, A_t))$. Every C steps, we update the parameters of the target action-value function $\\tilde{Q}$ by setting $\\tilde{\\Theta} = \\tau*\\Theta + (1 - \\tau)*\\tilde{\\Theta}$.\n",
    "\n",
    "### The hyperparameters\n",
    "\n",
    "\tBUFFER_SIZE = int(1e5)                      # replay buffer size \t\n",
    "\tBATCH_SIZE = 64                             # minibatch size\n",
    "\tGAMMA = 0.99                                # discount factor\n",
    "\tTAU = 1e-3                                  # for soft update of target parameters\n",
    "\tLR = 5e-4                                   # learning rate\n",
    "\tC = 4                                       # how often to update the network\n",
    "\tnum_episodes = 2000                         # the number of episodes\n",
    "\teps_start = 1                               # the initial value of epsilon for the epsilon-greedy policy\n",
    "\teps_decay = 0.995                           # the rate of change of epsilon for each episode\n",
    "\teps_min = 0.01                              # the minimum value of epsilon\n",
    "\t\n",
    "\n",
    "### The model architecture of the deep neural network\n",
    "\n",
    "We use a 3-layer neural network which takes the 37-dimensional state as the input and outputs the predicted Q-values for each of the 4 actions. All the layers are fully connected. The first layer has 64 nodes and uses a ReLU-activation function. The second layer has 64 nodes and uses a ReLU-activation function as well. The third layer has 4 nodes which correspond to the number of possible actions. We don't use an activation function here as we want to predict the discounted sum of future rewards. Here is the implementation in PyTorch:\n",
    "```python\n",
    "class QNetwork(nn.Module):\n",
    "   \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        x = self.fc1(state)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "                \n",
    "        return x\n",
    "```\n",
    "### The performance of the learning algorithm\n",
    "\n",
    "The algorithm at hand achieves an average reward of 13 over 100 consecutive episodes in 553 episodes. \n",
    "\n",
    "![](score.png) \n",
    "\n",
    "\n",
    "### Ideas for future work\n",
    "\n",
    "The performance could be improved by using the following methods from the literature: Double DQN[^3], Dueling DQN[^4] or Prioritized Experience Replay[^5].\n",
    "\n",
    "\n",
    "[^1]: Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S. & Hassabis, D. (2015). Human-level control through deep reinforcement learning. _Nature_, 518, 529--533.\n",
    "\n",
    "[^2]: Kingma, D. P. & Ba, J. (2014). Adam: A Method for Stochastic Optimization (cite arxiv:1412.6980 Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015)\n",
    "\n",
    "[^3]: van Hasselt, H., Guez, A. & Silver, D. (2015). Deep Reinforcement Learning with Double Q-learning. arXiv:1509.06461v3\n",
    "\n",
    "[^4]: Wang, Z., de Freitas, N. & Lanctot, M. (2015). Dueling Network Architectures for Deep Reinforcement Learning.. _CoRR_, abs/1511.06581.\n",
    "\n",
    "[^5]: Schaul, T., Quan, J., Antonoglou, I. & Silver, D. (2015). Prioritized Experience Replay (cite arxiv:1511.05952Comment: Published at ICLR 2016)\n",
    "\n",
    "> Written with [StackEdit](https://stackedit.io/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
